{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import socket\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "torch.manual_seed(777)\n",
    "if device ==\"cuda:0\":\n",
    "    torch.cuda.manual_seed_all(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (open(\"recsys_data/data.p\", \"rb\")) as openfile:\n",
    "    train_data = pickle.load(openfile)\n",
    "with (open(\"recsys_data/lab_ratings.p\", \"rb\")) as openfile:\n",
    "    lab_ratings = pickle.load(openfile)\n",
    "with (open(\"recsys_data/fb_data_train.p\", \"rb\")) as openfile:\n",
    "    fb_data_train = pickle.load(openfile)\n",
    "with (open(\"recsys_data/fb_labels_train.p\", \"rb\")) as openfile:\n",
    "    fb_labels_train = pickle.load(openfile)\n",
    "with (open(\"recsys_data/fb_data_test.p\", 'rb')) as openfile:\n",
    "    fb_data_test = pickle.load(openfile)\n",
    "with (open(\"recsys_data/fb_labels_test.p\", \"rb\")) as openfile:\n",
    "    fb_labels_test = pickle.load(openfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embed_file):\n",
    "    #also normalizes the embeddings\n",
    "    W = []\n",
    "    with open(embed_file) as ef:\n",
    "        for line in ef:\n",
    "            line = line.rstrip().split()\n",
    "            vec = np.array(line[1:]).astype(np.float)\n",
    "            vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "            W.append(vec)\n",
    "        #UNK embedding, gaussian randomly initialized \n",
    "        print(\"adding unk embedding\")\n",
    "        vec = np.random.randn(len(W[-1]))\n",
    "        vec = vec / float(np.linalg.norm(vec) + 1e-6)\n",
    "        W.append(vec)\n",
    "    W = np.array(W)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_FILE = 'recsys_data/processed_full.embed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding unk embedding\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = load_embeddings(EMBED_FILE)\n",
    "# embedding_matrix.shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51917"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "vocab = pd.read_csv('recsys_data/vocab.csv', header=None)\n",
    "len(set(sorted(vocab[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_dict(vocab_file):\n",
    "    vocab_df = pd.read_csv(vocab_file, header=None)\n",
    "    vocab = sorted(set(vocab_df[0].tolist()))\n",
    "    ind2w = {i+1:w for i,w in enumerate(vocab)}\n",
    "    w2ind = {w:i for i,w in ind2w.items()}\n",
    "    return ind2w, w2ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2w, w2idx = load_vocab_dict('recsys_data/vocab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    s = text.replace('[', \"\")\n",
    "    s = s.replace(']', \"\")\n",
    "    s = s.replace(\"'\", \"\")\n",
    "    s = s.replace(\",\", \"\")\n",
    "    s = s.split()\n",
    "    return s\n",
    "\n",
    "def encoding_disease(data, w2idx):\n",
    "    idx_total = []\n",
    "    for i in range(len(data['age'])):\n",
    "        text = data['disease'][i]\n",
    "        cleaned_text = clean_text(text)\n",
    "        idx = []\n",
    "        for st in cleaned_text:\n",
    "            if st not in w2idx:\n",
    "                idx.append(len(w2idx)+1)\n",
    "            else:\n",
    "                idx.append(w2idx[st])\n",
    "        idx_total.append(idx)\n",
    "    return np.array(idx_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, max_seq_len: int = 0):\n",
    "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
    "    # Pad\n",
    "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        padded_sequences[i][: len(sequence)] = sequence\n",
    "    return padded_sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-1e1034462875>:21: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return np.array(idx_total)\n"
     ]
    }
   ],
   "source": [
    "train_encoded_disease = encoding_disease(train_data, w2idx)\n",
    "train_padded_encoded_disease = pad_sequences(train_encoded_disease)\n",
    "fb_train_encoded_disease = encoding_disease(fb_data_train, w2idx)\n",
    "fb_train_padded_encoded_disease = pad_sequences(fb_train_encoded_disease)\n",
    "fb_test_encoded_disease = encoding_disease(fb_data_test, w2idx)\n",
    "fb_test_padded_encoded_disease = pad_sequences(fb_test_encoded_disease)\n",
    "\n",
    "train_data['encoded_disease'] = train_padded_encoded_disease\n",
    "fb_data_train['encoded_disease'] = fb_train_padded_encoded_disease\n",
    "fb_data_test['encoded_disease'] = fb_test_padded_encoded_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['age'] = train_data['age'].reshape((-1, 1))\n",
    "train_data['weight'] = train_data['weight'].reshape((-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_age = MinMaxScaler()\n",
    "scaler_weight = MinMaxScaler()\n",
    "train_data['age'] = scaler_age.fit_transform(train_data['age'])\n",
    "train_data['weight'] = scaler_weight.fit_transform(train_data['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = train_data\n",
    "        self.ratings = lab_ratings\n",
    "    def __getitem__(self, index):\n",
    "        age = self.data['age'][index]\n",
    "        weight = self.data['weight'][index]\n",
    "        icd_codes = self.data['codes'][index]\n",
    "        disease = self.data['encoded_disease'][index]\n",
    "        target = self.ratings[index]\n",
    "        return {\n",
    "            'age': torch.tensor(age, dtype=float),\n",
    "            'weight': torch.tensor(weight, dtype=float),\n",
    "            'disease': torch.tensor(disease, dtype=torch.long),\n",
    "            'icd_codes': torch.tensor(icd_codes, dtype=float),\n",
    "            'target': torch.tensor(target, dtype=float)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data['hadm_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_traindata = len(fb_labels_train) // user\n",
    "client_order = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_train['age'] = fb_data_train['age'].reshape((-1, 1))\n",
    "fb_data_train['weight'] = fb_data_train['weight'].reshape((-1, 1))\n",
    "\n",
    "scaler_age = MinMaxScaler()\n",
    "scaler_weight = MinMaxScaler()\n",
    "fb_data_train['age'] = scaler_age.fit_transform(fb_data_train['age'])[num_traindata * client_order : num_traindata * (client_order + 1)]\n",
    "fb_data_train['weight'] = scaler_weight.fit_transform(fb_data_train['weight'])[num_traindata * client_order : num_traindata * (client_order + 1)]\n",
    "fb_data_train['codes'] = fb_data_train['codes'][num_traindata * client_order : num_traindata * (client_order + 1)]\n",
    "fb_labels_train = fb_labels_train[num_traindata * client_order : num_traindata * (client_order + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_data_test['age'] = fb_data_test['age'].reshape((-1, 1))\n",
    "fb_data_test['weight'] = fb_data_test['weight'].reshape((-1, 1))\n",
    "\n",
    "scaler_age = MinMaxScaler()\n",
    "scaler_weight = MinMaxScaler()\n",
    "fb_data_test['age'] = scaler_age.fit_transform(fb_data_test['age'])\n",
    "fb_data_test['weight'] = scaler_weight.fit_transform(fb_data_test['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedbackDataset(Dataset):\n",
    "    def __init__(self, train=False):\n",
    "        if train:\n",
    "            self.data = fb_data_train\n",
    "            self.ratings = fb_labels_train\n",
    "        else:\n",
    "            self.data = fb_data_test\n",
    "            self.ratings = fb_labels_test\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        age = self.data['age'][index]\n",
    "        weight = self.data['weight'][index]\n",
    "        icd_codes = self.data['codes'][index]\n",
    "        disease = self.data['encoded_disease'][index]\n",
    "        target = self.ratings[index]\n",
    "        return {\n",
    "            'age': torch.tensor(age, dtype=float),\n",
    "            'weight': torch.tensor(weight, dtype=float),\n",
    "            'disease': torch.tensor(disease, dtype=torch.long),\n",
    "            'icd_codes': torch.tensor(icd_codes, dtype=float),\n",
    "            'target': torch.tensor(target, dtype=float)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.data['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_dataset = InitialDataset()\n",
    "train_loader = DataLoader(initial_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "fb_train_dataset = FeedbackDataset(train=True)\n",
    "fb_train_loader = DataLoader(fb_train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "fb_test_dataset = FeedbackDataset(train=False)\n",
    "fb_test_loader = DataLoader(fb_test_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(train_loader)\n",
    "print(total_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecSysClient(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "#         W = torch.Tensor(load_embeddings(EMBED_FILE))\n",
    "#         self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
    "#         self.embed.weight.data = W.clone()\n",
    "        W = torch.Tensor(embedding_matrix)\n",
    "        self.embed = nn.Embedding(W.size()[0], W.size()[1], padding_idx=0)\n",
    "        self.embed.weight.data = W.clone()\n",
    "        \n",
    "        self.fc1 = nn.Linear(152, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "#         self.fc3 = nn.Linear(512, 256)\n",
    "#         self.fc4 = nn.Linear(256, 25)\n",
    "    def forward(self, age, weight, icd_codes, disease):\n",
    "#         age = x['age']\n",
    "#         weight = x['weight']\n",
    "#         icd_codes = x['icd_codes']\n",
    "        \n",
    "        embedded = self.embed(disease)\n",
    "        embedded = torch.mean(embedded, 1)\n",
    "        \n",
    "        x = torch.cat((embedded, icd_codes, age, weight), 1).float()\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = RecSysClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 1\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.001\n",
    "optimizer = Adam(client.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_msg(sock, msg):\n",
    "    # prefix each message with a 4-byte length in network byte order\n",
    "    msg = pickle.dumps(msg)\n",
    "    msg = struct.pack('>I', len(msg)) + msg\n",
    "    sock.sendall(msg)\n",
    "\n",
    "def recv_msg(sock):\n",
    "    # read message length and unpack it into an integer\n",
    "    raw_msglen = recvall(sock, 4)\n",
    "    if not raw_msglen:\n",
    "        return None\n",
    "    msglen = struct.unpack('>I', raw_msglen)[0]\n",
    "    # read the message data\n",
    "    msg =  recvall(sock, msglen)\n",
    "    msg = pickle.loads(msg)\n",
    "    return msg\n",
    "\n",
    "def recvall(sock, n):\n",
    "    # helper function to receive n bytes or return None if EOF is hit\n",
    "    data = b''\n",
    "    while len(data) < n:\n",
    "        packet = sock.recv(n - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = '10.10.7.64'\n",
    "port = 10080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = socket.socket()\n",
    "s.connect((host, port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = recv_msg(s)   # get epoch\n",
    "msg = total_batch\n",
    "send_msg(s, msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 23.65it/s]\n",
      "Epoch 2: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.18it/s]\n",
      "Epoch 3: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.89it/s]\n",
      "Epoch 4: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.02it/s]\n",
      "Epoch 5: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.36it/s]\n",
      "Epoch 6: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.20it/s]\n",
      "Epoch 7: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.66it/s]\n",
      "Epoch 8: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.71it/s]\n",
      "Epoch 9: 100%|████████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.54it/s]\n",
      "Epoch 10: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.25it/s]\n",
      "Epoch 11: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.36it/s]\n",
      "Epoch 12: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.11it/s]\n",
      "Epoch 13: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.95it/s]\n",
      "Epoch 14: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.26it/s]\n",
      "Epoch 15: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.42it/s]\n",
      "Epoch 16: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.89it/s]\n",
      "Epoch 17: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.04it/s]\n",
      "Epoch 18: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.74it/s]\n",
      "Epoch 19: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.48it/s]\n",
      "Epoch 20: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 26.02it/s]\n",
      "Epoch 21: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.58it/s]\n",
      "Epoch 22: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.47it/s]\n",
      "Epoch 23: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.83it/s]\n",
      "Epoch 24: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.36it/s]\n",
      "Epoch 25: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.48it/s]\n",
      "Epoch 26: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 26.00it/s]\n",
      "Epoch 27: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.98it/s]\n",
      "Epoch 28: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.77it/s]\n",
      "Epoch 29: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.98it/s]\n",
      "Epoch 30: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.92it/s]\n",
      "Epoch 31: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.75it/s]\n",
      "Epoch 32: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.35it/s]\n",
      "Epoch 33: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.32it/s]\n",
      "Epoch 34: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.30it/s]\n",
      "Epoch 35: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.74it/s]\n",
      "Epoch 36: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 25.95it/s]\n",
      "Epoch 37: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.64it/s]\n",
      "Epoch 38: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 26.11it/s]\n",
      "Epoch 39: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.55it/s]\n",
      "Epoch 40: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.48it/s]\n",
      "Epoch 41: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 26.02it/s]\n",
      "Epoch 42: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.84it/s]\n",
      "Epoch 43: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 25.98it/s]\n",
      "Epoch 44: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.29it/s]\n",
      "Epoch 45: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.47it/s]\n",
      "Epoch 46: 100%|███████████████████████████████████████████████████| 181/181 [00:06<00:00, 26.41it/s]\n",
      "Epoch 47: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.49it/s]\n",
      "Epoch 48: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 24.75it/s]\n",
      "Epoch 49: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.42it/s]\n",
      "Epoch 50: 100%|███████████████████████████████████████████████████| 181/181 [00:07<00:00, 25.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    client_weights = recv_msg(s)\n",
    "    client.load_state_dict(client_weights)\n",
    "    client.eval()\n",
    "    for i, data in enumerate(tqdm(train_loader, ncols=100, desc='Epoch '+str(e+1))):\n",
    "        age, weight, icd_codes, disease = data['age'], data['weight'], data['icd_codes'], data['disease']\n",
    "        target = data['target']\n",
    "        \n",
    "        age = age.to(device)\n",
    "        weight = weight.to(device)\n",
    "        icd_codes = icd_codes.to(device)\n",
    "        disease = disease.to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = client(age, weight, icd_codes, disease)\n",
    "        client_output = output.clone().detach().requires_grad_(True)\n",
    "        msg = {\n",
    "            'client_output': client_output,\n",
    "            'label': target\n",
    "        }\n",
    "        send_msg(s, msg)\n",
    "        client_grad = recv_msg(s)\n",
    "        output.backward(client_grad)\n",
    "        optimizer.step()\n",
    "    send_msg(s, client.state_dict())\n",
    "    time.sleep(0.5)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = socket.socket()\n",
    "# s.connect((host, port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batch = len(fb_train_loader)\n",
    "epoch = recv_msg(s)   # get epoch\n",
    "msg = total_batch\n",
    "send_msg(s, msg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.09it/s]\n",
      "Epoch 2: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.54it/s]\n",
      "Epoch 3: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.94it/s]\n",
      "Epoch 4: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.02it/s]\n",
      "Epoch 5: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.10it/s]\n",
      "Epoch 6: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.55it/s]\n",
      "Epoch 7: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.71it/s]\n",
      "Epoch 8: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 24.43it/s]\n",
      "Epoch 9: 100%|██████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.57it/s]\n",
      "Epoch 10: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.80it/s]\n",
      "Epoch 11: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.85it/s]\n",
      "Epoch 12: 100%|█████████████████████████████████████████████████████| 73/73 [00:03<00:00, 22.97it/s]\n",
      "Epoch 13: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.42it/s]\n",
      "Epoch 14: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.06it/s]\n",
      "Epoch 15: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.55it/s]\n",
      "Epoch 16: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.19it/s]\n",
      "Epoch 17: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 24.49it/s]\n",
      "Epoch 18: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.77it/s]\n",
      "Epoch 19: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.58it/s]\n",
      "Epoch 20: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 24.42it/s]\n",
      "Epoch 21: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.00it/s]\n",
      "Epoch 22: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.90it/s]\n",
      "Epoch 23: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.67it/s]\n",
      "Epoch 24: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 25.36it/s]\n",
      "Epoch 25: 100%|█████████████████████████████████████████████████████| 73/73 [00:03<00:00, 23.46it/s]\n",
      "Epoch 26: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.78it/s]\n",
      "Epoch 27: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.15it/s]\n",
      "Epoch 28: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.92it/s]\n",
      "Epoch 29: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.24it/s]\n",
      "Epoch 30: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.13it/s]\n",
      "Epoch 31: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.35it/s]\n",
      "Epoch 32: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.37it/s]\n",
      "Epoch 33: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.03it/s]\n",
      "Epoch 34: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.51it/s]\n",
      "Epoch 35: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.24it/s]\n",
      "Epoch 36: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.04it/s]\n",
      "Epoch 37: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 24.55it/s]\n",
      "Epoch 38: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.22it/s]\n",
      "Epoch 39: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.31it/s]\n",
      "Epoch 40: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.53it/s]\n",
      "Epoch 41: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.90it/s]\n",
      "Epoch 42: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.91it/s]\n",
      "Epoch 43: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.35it/s]\n",
      "Epoch 44: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.22it/s]\n",
      "Epoch 45: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.24it/s]\n",
      "Epoch 46: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.90it/s]\n",
      "Epoch 47: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.78it/s]\n",
      "Epoch 48: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.17it/s]\n",
      "Epoch 49: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 26.41it/s]\n",
      "Epoch 50: 100%|█████████████████████████████████████████████████████| 73/73 [00:02<00:00, 27.16it/s]\n"
     ]
    }
   ],
   "source": [
    "for e in range(epoch):\n",
    "    client_weights = recv_msg(s)\n",
    "    client.load_state_dict(client_weights)\n",
    "    client.eval()\n",
    "    for i, data in enumerate(tqdm(fb_train_loader, ncols=100, desc='Epoch '+str(e+1))):\n",
    "        age, weight, icd_codes, disease = data['age'], data['weight'], data['icd_codes'], data['disease']\n",
    "        target = data['target']\n",
    "        \n",
    "        age = age.to(device)\n",
    "        weight = weight.to(device)\n",
    "        icd_codes = icd_codes.to(device)\n",
    "        disease = disease.to(device)\n",
    "        \n",
    "        target = target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = client(age, weight, icd_codes, disease)\n",
    "        client_output = output.clone().detach().requires_grad_(True)\n",
    "        msg = {\n",
    "            'client_output': client_output,\n",
    "            'label': target\n",
    "        }\n",
    "        send_msg(s, msg)\n",
    "        client_grad = recv_msg(s)\n",
    "        output.backward(client_grad)\n",
    "        optimizer.step()\n",
    "    send_msg(s, client.state_dict())\n",
    "    time.sleep(0.5)       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
